{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# from src.huffman import *\n",
    "from src.channel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import batched # TO-DO: fix not working with current python version, +3.12 needed\n",
    "\n",
    "import queue\n",
    "\n",
    "def batched_manual(iterable, n):\n",
    "    # A simple manual implementation of batched\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == n:\n",
    "            yield tuple(batch)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield tuple(batch)\n",
    "\n",
    "def build_huffman_tree(imarray, n=1):\n",
    "    # Count element frequencies\n",
    "    element_counts = {}\n",
    "    for element in batched_manual(imarray.flatten(), n):\n",
    "        if element in element_counts:\n",
    "            element_counts[element] += 1\n",
    "        else:\n",
    "            element_counts[element] = 1\n",
    "\n",
    "    # Build priority queue to apply Huffman algorithm\n",
    "    id = {}\n",
    "    idr = {}\n",
    "    q = queue.PriorityQueue()\n",
    "    for key, value in element_counts.items():\n",
    "        print(f\"{key}, \")\n",
    "        id[key] = len(id)\n",
    "        idr[id[key]] = key\n",
    "        q.put((value, [id[key]]))\n",
    "\n",
    "    # Build Huffman tree (transf maps id to reversed code path)\n",
    "    transf = {}\n",
    "    while q.qsize() > 1:\n",
    "        a = q.get()\n",
    "        b = q.get()\n",
    "        for i in a[1]:\n",
    "            if i in transf:\n",
    "                transf[i] += '1'\n",
    "            else:\n",
    "                transf[i] = '1'\n",
    "        for i in b[1]:\n",
    "            if i in transf:\n",
    "                transf[i] += '0'\n",
    "            else:\n",
    "                transf[i] = '0'\n",
    "        new_val = (a[0] + b[0], a[1] + b[1])\n",
    "        q.put(new_val)\n",
    "\n",
    "    # Reverse Huffman tree paths to get actual codes (transf maps id to code)\n",
    "    for key, path in transf.items():\n",
    "        transf[key] = path[::-1]\n",
    "\n",
    "    # Build decoding dictionary mapping code string to original pixel tuple\n",
    "    code_to_pixel_tuple = {}\n",
    "    # Assuming 'id' maps pixel tuples to element IDs, and 'transf' maps element IDs to codes\n",
    "    for pixel_tuple, element_id in id.items():\n",
    "         code = transf[element_id]\n",
    "         code_to_pixel_tuple[code] = pixel_tuple\n",
    "\n",
    "    return transf, id, idr, code_to_pixel_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 14  # codeword length\n",
    "k = 10   # source word length\n",
    "EbfN0 = 10  # Signal to noise ratio (in times)\n",
    "\n",
    "# Proposed parity matrix\n",
    "P = np.matrix([[1, 1, 0, 0],\n",
    "     [0, 1, 1, 0],\n",
    "     [0, 0, 1, 1],\n",
    "     [1, 0, 0, 1],\n",
    "     [1, 1, 1, 0],\n",
    "     [0, 1, 1, 1],\n",
    "     [1, 0, 1, 1],\n",
    "     [1, 1, 0, 1],\n",
    "     [1, 1, 1, 1],\n",
    "     [1, 0, 1, 0]], dtype=np.int64)\n",
    "G = np.hstack((np.eye(k), P))\n",
    "H = np.hstack((P.T, np.eye(n-k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True), \n",
      "(True, False), \n",
      "(False, False), \n",
      "(False, True), \n",
      "434\n",
      "142171\n"
     ]
    }
   ],
   "source": [
    "im = Image.open('../imgs/logo.tif')\n",
    "imarray = np.array(im)\n",
    "\n",
    "x = 2\n",
    "\n",
    "transf, id, idr, code_to_pixel_tuple = build_huffman_tree(imarray, x)\n",
    "\n",
    "cimarray = ''\n",
    "for element in batched_manual(imarray.flatten(), x):\n",
    "    cimarray += transf[id[element]]\n",
    "\n",
    "print(len(imarray))\n",
    "print(len(cimarray))\n",
    "\n",
    "# Convert the string of bits into a list of integers (0 or 1)\n",
    "cimarray_list = [int(bit) for bit in cimarray]\n",
    "\n",
    "# Convert the list into a numpy matrix (1xN row vector)\n",
    "aux = np.matrix(cimarray_list)\n",
    "\n",
    "decoded_image_bits = []\n",
    "\n",
    "# Get the total number of bits\n",
    "total_bits = aux.shape[1]\n",
    "\n",
    "# Calculate the number of padding bits needed\n",
    "padding_bits = (k - (total_bits % k)) % k\n",
    "\n",
    "# Pad the aux matrix if necessary\n",
    "if padding_bits > 0:\n",
    "    padding = np.zeros((1, padding_bits), dtype=int)\n",
    "    aux = np.concatenate((aux, padding), axis=1)\n",
    "\n",
    "\n",
    "max_retransmissions = 100 # Set a maximum number of retransmissions to prevent infinite loops\n",
    "\n",
    "# Process aux in chunks of size k\n",
    "for i in range(0, aux.shape[1], k):\n",
    "    message_chunk = aux[:, i:i+k] # This should be 1x10\n",
    "    original_encoded_codeword = encode_message(message_chunk, G) # This should be 1x14\n",
    "\n",
    "    received_codeword = None\n",
    "    syndrome = np.matrix(np.ones((1, n - k), dtype=int)) # Initialize with non-zero syndrome to enter loop\n",
    "    retransmission_count = 0\n",
    "\n",
    "    # Simulate retransmissions until no error is detected or max retransmissions reached\n",
    "    while np.any(syndrome) and retransmission_count < max_retransmissions:\n",
    "        received_codeword = noisy_channel(original_encoded_codeword, n=n, k=k, EbfN0=EbfN0) # This should be 1x14\n",
    "        syndrome = (received_codeword @ H.transpose()) % 2 # H.transpose() is 14x4, result is 1x4\n",
    "\n",
    "        if np.any(syndrome):\n",
    "            retransmission_count += 1 # Error detected, simulate retransmission\n",
    "\n",
    "    # After successfully receiving a codeword (syndrome is zero) or max retransmissions reached\n",
    "    if not np.any(syndrome):\n",
    "        # This assumes the encode_message produces a systematic codeword\n",
    "        decoded_message_chunk = received_codeword[:, :k] # Extract first k bits (1x10)\n",
    "        decoded_image_bits.append(decoded_message_chunk)\n",
    "    else:\n",
    "        # If max retransmissions reached and error still detected, what to do?\n",
    "        # Option 1: Append a block of zeros (treat as erasure)\n",
    "        decoded_image_bits.append(np.zeros((1, k), dtype=int))\n",
    "        print(f\"  Max retransmissions ({max_retransmissions}) reached for chunk starting at bit {i}. Appending zero block.\")\n",
    "        # Option 2: Append the last received (erroneous) message part (might introduce more errors)\n",
    "        # decoded_message_chunk = received_codeword[:, :k]\n",
    "        # decoded_image_bits.append(decoded_message_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed array size: 187488\n",
      "Original image size: 187488\n",
      "Reconstructed array first 20 values: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Reconstructed array max value: 1\n",
      "Reconstructed array min value: 0\n",
      "Reconstructed array mean value: 0.5146142686465267\n",
      "Image decoded and saved as 'decoded.tif'\n"
     ]
    }
   ],
   "source": [
    "# At this point, decoded_image_bits is a list of numpy matrices, each 1xk.\n",
    "combined_decoded_bits = np.concatenate(decoded_image_bits, axis=1)\n",
    "\n",
    "# Unpad combined_decoded_bits to get the original number of bits in cimarray\n",
    "# total_bits = \"length of aux before padding\"\n",
    "combined_decoded_bits = combined_decoded_bits[:, :total_bits]\n",
    "decoded_bits_array = np.array(combined_decoded_bits).flatten()\n",
    "\n",
    "# Now, perform inverse Huffman decoding to get back the pixel values\n",
    "reconstructed_pixels = []\n",
    "current_code_bits = \"\"\n",
    "# Now, perform inverse Huffman decoding to get back the pixel values\n",
    "reconstructed_pixels = []\n",
    "current_code_bits = \"\"\n",
    "\n",
    "# Use the code_to_pixel_tuple dictionary for decoding\n",
    "# Iterate through the flattened decoded bits array\n",
    "for bit in decoded_bits_array:\n",
    "    current_code_bits += str(bit)\n",
    "    # Check if the current accumulated bits form a valid Huffman code\n",
    "    if current_code_bits in code_to_pixel_tuple:\n",
    "        # Get the original pixel tuple for the found code\n",
    "        pixel_tuple = code_to_pixel_tuple[current_code_bits]\n",
    "        # Add the pixel values from the tuple to our list\n",
    "        reconstructed_pixels.extend(pixel_tuple)\n",
    "        # Reset the accumulated bits for the next code\n",
    "        current_code_bits = \"\"\n",
    "\n",
    "# Convert the list of reconstructed pixels to a numpy array\n",
    "reconstructed_array = np.array(reconstructed_pixels, dtype=np.uint8)\n",
    "\n",
    "# Check if the number of reconstructed pixels matches the original image size\n",
    "# If not, pad or truncate the reconstructed array to match the original size\n",
    "# This is necessary for reshaping but note that a size mismatch indicates errors\n",
    "if reconstructed_array.size < imarray.size:\n",
    "    padding_size = imarray.size - reconstructed_array.size\n",
    "    reconstructed_array = np.pad(reconstructed_array, (0, padding_size), 'constant', constant_values=0)\n",
    "    print(f\"Padded reconstructed pixels with {padding_size} zeros to match original image size.\")\n",
    "elif reconstructed_array.size > imarray.size:\n",
    "     reconstructed_array = reconstructed_array[:imarray.size]\n",
    "     print(f\"Truncated reconstructed pixels by {reconstructed_array.size - imarray.size} to match original image size.\")\n",
    "\n",
    "\n",
    "print(f\"Reconstructed array size: {reconstructed_array.size}\")\n",
    "print(f\"Original image size: {imarray.size}\")\n",
    "print(f\"Reconstructed array first 20 values: {reconstructed_array[:20]}\")\n",
    "print(f\"Reconstructed array max value: {np.max(reconstructed_array) if reconstructed_array.size > 0 else 'N/A'}\")\n",
    "print(f\"Reconstructed array min value: {np.min(reconstructed_array) if reconstructed_array.size > 0 else 'N/A'}\")\n",
    "print(f\"Reconstructed array mean value: {np.mean(reconstructed_array) if reconstructed_array.size > 0 else 'N/A'}\")\n",
    "\n",
    "# Count occurrences of values (optional, but can be helpful)\n",
    "# unique_values, counts = np.unique(reconstructed_array, return_counts=True)\n",
    "# print(f\"Unique values and their counts in reconstructed_array: {list(zip(unique_values, counts))[:10]}...\") # print only first 10 unique values\n",
    "\n",
    "# Now, reshape the array back to the original image dimensions\n",
    "# This step will now succeed because the array size matches imarray.size\n",
    "decoded_image = reconstructed_array.reshape(imarray.shape)\n",
    "\n",
    "# Save the decoded image as a TIFF file\n",
    "from PIL import Image\n",
    "# Ensure the data type is correct for Image.fromarray, typically uint8 for grayscale/color images\n",
    "# Also, handle potential float values if padding introduced them, convert to int/uint8\n",
    "if decoded_image.dtype != np.uint8:\n",
    "    # Convert to uint8, clipping values if necessary\n",
    "    decoded_image = np.clip(decoded_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "decoded_image_pil = Image.fromarray(decoded_image)\n",
    "decoded_image_pil.save('../imgs/decoded.tif')\n",
    "print(\"Image decoded and saved as 'decoded.tif'\")\n",
    "\n",
    "# You might still want to print the original reconstructed_array.size to see the extent of the mismatch\n",
    "# print(f\"Number of pixels reconstructed before padding/truncating: {np.array(reconstructed_pixels).size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.ones((100, 100), dtype=np.uint8) * 255  # Multiply by 255 to get white pixels\n",
    "example_tif = Image.fromarray(example)\n",
    "example_tif.save('../imgs/decoded_white.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
